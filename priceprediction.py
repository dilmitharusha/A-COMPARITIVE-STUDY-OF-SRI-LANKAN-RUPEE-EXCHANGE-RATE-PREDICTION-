# -*- coding: utf-8 -*-
"""PricePrediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-HpO1xGtX4n14NZfb8IggMX7b9T37Tpi
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.dates as mdates
# %matplotlib inline
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import keras
import seaborn as sns
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.svm import SVR
from sklearn.model_selection import GridSearchCV


from google.colab import drive

#mounting google drive
drive.mount('/content/drive')

#Reading dataset
data = pd.read_csv('/content/drive/MyDrive/MathsProject/USD_LKR Historical Data.csv')

data.index=pd.to_datetime(data['Date'])
data.index = data.index.date

data

#removing unwanted columns from dataset
del data['Change %']
del data['Date']

#validation dataset
data=data.iloc[:-100]
data_val=data.iloc[-100:]

#making column headers lower case in order to easy use
data.columns=[c.lower() for c in data.columns]
data_val.columns=[c.lower() for c in data_val.columns]

plt.figure()
data.plot(subplots=True, figsize=(10,10))
plt.ylabel("Price in Sri Lankan Rupees")
plt.xlabel("Date");

plt.tight_layout()

#checking for null values
null_values=data.isnull()
print(f'{null_values.sum()} Null values')
print(null_values)

# Removing rows with null values
data = data.dropna()
print("\nDataFrame after removing rows with null values:")
print(data)

# Removing rows with zero values
data = data[~(data == 0).any(axis=1)]
print("\nDataFrame after removing rows with zero values:")
print(data)

#checking outliers
sns.boxplot(data=data,palette="winter")
plt.title("Before Removing Outliers")



early_stopping = EarlyStopping(
    monitor='loss',   # Metric to monitor (usually validation loss)
    patience=5,          # Number of epochs with no improvement after which training will be stopped
    restore_best_weights=True   # Restore the best model weights when training stops
)

# Split the dataset into input features (X) and target variable (y)
X = data[['open', 'high', 'low',]]
y = data['price']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)

# Standardize the input features
scaler = StandardScaler()

# Scale the training data and testing data
X_train = scaler.fit_transform(X_train)
X_test = scaler.fit_transform(X_test)

X_train

X_train = X_train.reshape(X_train.shape[0], 1, X_train.shape[1])
X_test = X_test.reshape(X_test.shape[0], 1, X_test.shape[1])

"""LSTM MODEL"""

model = keras.Sequential()
model.add(keras.layers.LSTM(50, input_shape=(1, X_train.shape[2])))
model.add(keras.layers.Dense(1))

model.compile(optimizer='adam', loss='mse')
# model.summary()

model.fit(X_train, y_train, epochs=150, batch_size=4, callbacks=[early_stopping])

y_pred_lstm = model.predict(X_test)

# Calculate and print evaluation metrics
mse = mean_squared_error(y_test, y_pred_lstm)
mae = mean_absolute_error(y_test, y_pred_lstm)
r2 = r2_score(y_test, y_pred_lstm)


print("Mean Squared Error:", mse)
print("Mean Absolute Error:", mae)
print("R-squared:", r2)

# Save the LSTM model to an H5 file
model.save("lstm_model.h5")

y_test[:10]

y_pred_lstm[:10]

# Create a DataFrame with date indices from y_test
results_df = pd.DataFrame(index=y_test.index)

results_df['y_test'] = y_test.values
results_df['y_pred'] = y_pred_lstm

print(results_df)



"""ANN MODEL"""

# Creating a ANN model
model = keras.Sequential([
    layers.Dense(128, activation='relu', input_shape=(X_train.shape[1],)),
    layers.Dropout(0.2),  # Adding dropout to the first layer
    # layers.Dense(64, activation='relu', kernel_regularizer=keras.regularizers.l2(0.01)),  # L2 regularization
    # layers.Dropout(0.1),  # Adding dropout to the second layer
    layers.Dense(1)
])

# Compile the model
model.compile(optimizer='adam', loss='mean_squared_error')

early_stopping = EarlyStopping(
    monitor='loss',
    patience=5,
    restore_best_weights=True
)

# Train the model on the training dataset
trained_model=model.fit(X_train, y_train, epochs=100, batch_size=16, validation_data=(X_test, y_test), callbacks=[early_stopping])

# Make predictions on the test data
y_pred_ann = model.predict(X_test)

# Calculate and print evaluation metrics
mse = mean_squared_error(y_test, y_pred_ann)
mae = mean_absolute_error(y_test, y_pred_ann)
r2 = r2_score(y_test, y_pred_ann)
print(f"Mean Squared Error: {mse:.3f}")
print(f"Mean Absolute Error: {mae:.3f}")
print(f"R-squared: {r2:.3f}")

# Save the ANN model to an H5 file
model.save("ann_model.h5")

y_test[:10]

y_pred_ann[:10]

# Create a DataFrame with date indices from y_test
results_df = pd.DataFrame(index=y_test.index)

results_df['y_test'] = y_test.values
results_df['y_pred'] = y_pred_ann

print(results_df)

"""SVR MODEL"""

# Define a grid of hyperparameters to search
param_grid = {
    'C': [1e3],  # Regularization parameter
    'kernel': [ 'rbf'],  # Kernel type
    'gamma': [ 0.00001]  # Kernel coefficient (only for 'rbf' and 'poly' kernels)
}

# Create an SVR model
svr = SVR()

# Perform grid search with cross-validation
grid_search = GridSearchCV(svr, param_grid, cv=5)  # 5-fold cross-validation
grid_search.fit(X_train, y_train)

# Get the best hyperparameters
best_params = grid_search.best_params_
best_svr = grid_search.best_estimator_

# Create an SVR model with the best hyperparameters
svr_model = SVR(C=best_params['C'], kernel=best_params['kernel'], gamma=best_params['gamma'])

# Fit the SVR model on the training data
svr_model.fit(X_train, y_train)

# Predict on the test set
y_test_pred = svr_model.predict(X_test)

from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

# Evaluate on the test set
test_mse = mean_squared_error(y_test, y_test_pred)
test_mae = mean_absolute_error(y_test, y_test_pred)
test_r2 = r2_score(y_test, y_test_pred)

print("Test Set Metrics:")
print(f"Mean Squared Error (MSE): {test_mse:.2f}")
print(f"Mean Absolute Error (MAE): {test_mae:.2f}")
print(f"R-squared (R2): {test_r2:.2f}")

y_test[:10]

y_test_pred[:10]

# Create a DataFrame with date indices from y_test
results_df = pd.DataFrame(index=y_test.index)

results_df['y_test'] = y_test.values
results_df['y_pred'] = y_test_pred

print(results_df)

import matplotlib.pyplot as plt

# Create a scatter plot to compare actual vs. predicted values
plt.figure(figsize=(8, 6))
plt.scatter(y_test, y_test_pred, color='blue', label='Actual vs. Predicted')
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2, color='red', label='Perfectly Predicted')
plt.xlabel('Actual Prices')
plt.ylabel('Predicted Prices')
plt.title('Actual vs. Predicted Prices (SVR Model)')
plt.legend()
plt.show()